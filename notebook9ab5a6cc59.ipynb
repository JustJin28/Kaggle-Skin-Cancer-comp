{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport cv2\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.utils import resample\nfrom sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Concatenate, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.applications.imagenet_utils import preprocess_input\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.metrics import Precision, Recall, AUC\nimport h5py\nimport random\nimport base64\nimport io\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-19T13:57:31.134373Z","iopub.execute_input":"2024-08-19T13:57:31.134921Z","iopub.status.idle":"2024-08-19T13:57:43.763316Z","shell.execute_reply.started":"2024-08-19T13:57:31.134878Z","shell.execute_reply":"2024-08-19T13:57:43.762545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set random seed for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:57:43.765298Z","iopub.execute_input":"2024-08-19T13:57:43.766048Z","iopub.status.idle":"2024-08-19T13:57:43.770706Z","shell.execute_reply.started":"2024-08-19T13:57:43.766013Z","shell.execute_reply":"2024-08-19T13:57:43.769670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# File paths\ntrain_metadata_path = '/kaggle/input/isic-2024-challenge/train-metadata.csv'\ntrain_image_hdf5_path = '/kaggle/input/isic-2024-challenge/train-image.hdf5'\ntest_metadata_path = '/kaggle/input/isic-2024-challenge/test-metadata.csv'\ntest_image_hdf5_path = '/kaggle/input/isic-2024-challenge/test-image.hdf5'","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:57:43.772079Z","iopub.execute_input":"2024-08-19T13:57:43.772413Z","iopub.status.idle":"2024-08-19T13:57:43.795312Z","shell.execute_reply.started":"2024-08-19T13:57:43.772381Z","shell.execute_reply":"2024-08-19T13:57:43.794558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_image_from_hdf5(hdf5_path, image_id):\n    with h5py.File(hdf5_path, 'r') as hdf:\n        # Load the raw data\n        image_data = hdf[image_id][()]\n        \n    # Convert the data to a numpy array\n    image_array = np.frombuffer(image_data, dtype=np.uint8)\n    \n    # Decode the image using OpenCV\n    image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n    \n    # Convert BGR to RGB (OpenCV loads images in BGR format)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    return image","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:57:43.796217Z","iopub.execute_input":"2024-08-19T13:57:43.796448Z","iopub.status.idle":"2024-08-19T13:57:43.804983Z","shell.execute_reply.started":"2024-08-19T13:57:43.796427Z","shell.execute_reply":"2024-08-19T13:57:43.804200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_image(image, target_size=(224, 224)):\n    # Resize the image\n    image_resized = cv2.resize(image, target_size)\n    \n    # Normalize the image\n    image_normalized = image_resized.astype(np.float32) / 255.0\n    \n    return image_normalized","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:57:43.807390Z","iopub.execute_input":"2024-08-19T13:57:43.807835Z","iopub.status.idle":"2024-08-19T13:57:43.817166Z","shell.execute_reply.started":"2024-08-19T13:57:43.807812Z","shell.execute_reply":"2024-08-19T13:57:43.816309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_and_preprocess_data(metadata_path, hdf5_path, is_train=True):\n    # Load metadata\n    data = pd.read_csv(metadata_path, low_memory=False)\n    \n    # Drop unnecessary columns\n    columns_to_drop = ['patient_id', 'copyright_license', 'attribution', 'image_type', \n                       'tbp_tile_type']\n    if is_train:\n        columns_to_drop.append('lesion_id')\n    data = data.drop(columns=columns_to_drop, errors='ignore')\n    \n    # Handle missing values in numeric columns\n    numeric_columns = ['age_approx', 'clin_size_long_diam_mm', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext', \n                       'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', 'tbp_lv_Lext', \n                       'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean', 'tbp_lv_deltaA', \n                       'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', \n                       'tbp_lv_minorAxisMM', 'tbp_lv_nevi_confidence', 'tbp_lv_norm_border', 'tbp_lv_norm_color', \n                       'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max', 'tbp_lv_stdL', 'tbp_lv_stdLExt', \n                       'tbp_lv_symm_2axis', 'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z']\n    \n    imputer = SimpleImputer(strategy='median')\n    data[numeric_columns] = imputer.fit_transform(data[numeric_columns])\n    \n    # Encode categorical variables\n    categorical_columns = ['sex', 'anatom_site_general', 'tbp_lv_location', 'tbp_lv_location_simple']\n    if is_train:\n        categorical_columns.extend(['iddx_full', 'iddx_1', 'iddx_2', 'iddx_3', 'iddx_4', 'iddx_5'])\n    \n    for col in categorical_columns:\n        if col in data.columns:  # Check if column exists\n            le = LabelEncoder()\n            data[col] = data[col].fillna('Unknown')  # Fill NaN with 'Unknown'\n            data[col] = le.fit_transform(data[col].astype(str))\n    \n    # One-hot encode relevant categorical variables\n    categorical_columns_to_onehot = ['sex', 'anatom_site_general', 'tbp_lv_location', 'tbp_lv_location_simple']\n    if is_train:\n        categorical_columns_to_onehot.append('iddx_1')\n    data = pd.get_dummies(data, columns=categorical_columns_to_onehot)\n    \n    # Scale numerical features\n    scaler = StandardScaler()\n    data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n    \n    if is_train:\n        # Handle 'mel_mitotic_index' if present\n        if 'mel_mitotic_index' in data.columns:\n            mitotic_index_mapping = {\n                '<1/mm^2': 0, '0/mm^2': 0, '1/mm^2': 1, '2/mm^2': 2, \n                '3/mm^2': 3, '4/mm^2': 4, '>4/mm^2': 5\n            }\n            data['mel_mitotic_index'] = data['mel_mitotic_index'].map(mitotic_index_mapping).fillna(-1)\n        \n        # Handle 'mel_thick_mm' if present\n        if 'mel_thick_mm' in data.columns:\n            data['mel_thick_mm'] = pd.to_numeric(data['mel_thick_mm'], errors='coerce').fillna(-1)\n        \n        # Ensure target is int\n        data['target'] = data['target'].astype(int)\n    \n    # Reset index\n    data = data.reset_index(drop=True)\n    \n    return data, hdf5_path","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:57:43.818467Z","iopub.execute_input":"2024-08-19T13:57:43.818775Z","iopub.status.idle":"2024-08-19T13:57:43.833157Z","shell.execute_reply.started":"2024-08-19T13:57:43.818753Z","shell.execute_reply":"2024-08-19T13:57:43.832096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class HDF5DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, data, hdf5_path, batch_size=32, dim=(224, 224), n_channels=3, shuffle=True):\n        self.data = data\n        self.hdf5_path = hdf5_path\n        self.batch_size = batch_size\n        self.dim = dim\n        self.n_channels = n_channels\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(np.ceil(len(self.data) / float(self.batch_size)))\n\n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        batch = self.data.iloc[indexes]\n        X, y = self.__data_generation(batch)\n        return ({'image_input': X[0], 'tabular_input': X[1]}, y)\n\n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.data))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, batch):\n        X_img = np.empty((len(batch), *self.dim, self.n_channels))\n        X_tab = np.empty((len(batch), len(self.data.columns) - 2))\n        y = np.empty((len(batch), 1), dtype=int)\n\n        for i, (_, row) in enumerate(batch.iterrows()):\n            img = load_image_from_hdf5(self.hdf5_path, row['isic_id'])\n            img_processed = preprocess_image(img, self.dim)\n            \n            X_img[i,] = img_processed\n            X_tab[i,] = row.drop(['isic_id', 'target']).values\n            y[i] = row['target']\n\n        return [X_img, X_tab], y","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:57:43.834325Z","iopub.execute_input":"2024-08-19T13:57:43.834666Z","iopub.status.idle":"2024-08-19T13:57:43.846751Z","shell.execute_reply.started":"2024-08-19T13:57:43.834636Z","shell.execute_reply":"2024-08-19T13:57:43.845977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Focal Loss implementation\ndef focal_loss(gamma=2., alpha=.25):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1 + K.epsilon())) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n    return focal_loss_fixed","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:57:43.847819Z","iopub.execute_input":"2024-08-19T13:57:43.848109Z","iopub.status.idle":"2024-08-19T13:57:43.860019Z","shell.execute_reply.started":"2024-08-19T13:57:43.848079Z","shell.execute_reply":"2024-08-19T13:57:43.859156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to balance dataset\ndef balance_dataset(data, undersample_ratio=0.5):\n    majority_class = data[data['target'] == 0]\n    minority_class = data[data['target'] == 1]\n    \n    # Undersample majority class\n    n_majority = int(len(minority_class) / (1 - undersample_ratio))\n    majority_undersampled = resample(majority_class, \n                                     n_samples=n_majority, \n                                     random_state=42)\n    \n    # Combine minority class with undersampled majority class\n    balanced_data = pd.concat([majority_undersampled, minority_class])\n    \n    return balanced_data.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:57:43.860947Z","iopub.execute_input":"2024-08-19T13:57:43.861197Z","iopub.status.idle":"2024-08-19T13:57:43.869791Z","shell.execute_reply.started":"2024-08-19T13:57:43.861166Z","shell.execute_reply":"2024-08-19T13:57:43.869045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(img_shape, tab_shape):\n    # Image input branch\n    img_input = Input(shape=img_shape, name='image_input')\n    x = Conv2D(32, (3, 3), activation='relu')(img_input)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D((2, 2))(x)\n    x = Conv2D(64, (3, 3), activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D((2, 2))(x)\n    x = Flatten()(x)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.3)(x)\n\n    # Tabular input branch\n    tab_input = Input(shape=(tab_shape,), name='tabular_input')\n    y = Dense(64, activation='relu')(tab_input)\n    y = BatchNormalization()(y)\n    y = Dropout(0.3)(y)\n\n    # Combine branches\n    combined = Concatenate()([x, y])\n    z = Dense(32, activation='relu')(combined)\n    z = BatchNormalization()(z)\n    z = Dropout(0.3)(z)\n    output = Dense(1, activation='sigmoid')(z)\n\n    model = Model(inputs=[img_input, tab_input], outputs=output)\n    model.compile(optimizer=Adam(learning_rate=1e-4),\n                  loss=focal_loss(alpha=.25, gamma=2),\n                  metrics=['accuracy', AUC(name='auc'), Precision(name='precision'), Recall(name='recall')])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:57:43.870865Z","iopub.execute_input":"2024-08-19T13:57:43.871171Z","iopub.status.idle":"2024-08-19T13:57:43.883115Z","shell.execute_reply.started":"2024-08-19T13:57:43.871143Z","shell.execute_reply":"2024-08-19T13:57:43.882415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(train_generator, val_generator=None, n_splits=5, epochs=50):\n    if val_generator is None:\n        # If no validation generator is provided, use a portion of the training data\n        train_data = train_generator.data\n        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n        best_model = None\n        best_auc = 0\n        \n        for fold, (train_idx, val_idx) in enumerate(skf.split(train_data, train_data['target'])):\n            print(f\"Training fold {fold + 1}\")\n            \n            train_data_fold = train_data.iloc[train_idx].reset_index(drop=True)\n            val_data_fold = train_data.iloc[val_idx].reset_index(drop=True)\n            \n            train_gen_fold = HDF5DataGenerator(train_data_fold, train_generator.hdf5_path)\n            val_gen_fold = HDF5DataGenerator(val_data_fold, train_generator.hdf5_path)\n            \n            model = create_model((224, 224, 3), train_data.shape[1] - 2)\n            \n            # Define callbacks\n            model_checkpoint = ModelCheckpoint(\n                f'best_model_fold_{fold+1}.keras',\n                monitor='val_auc',\n                mode='max',\n                save_best_only=True,\n                verbose=1\n            )\n            callbacks = [\n                EarlyStopping(patience=10, restore_best_weights=True),\n                ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6),\n                model_checkpoint\n            ]\n            \n            # Train model\n            history = model.fit(\n                train_gen_fold,\n                validation_data=val_gen_fold,\n                epochs=epochs,\n                callbacks=callbacks\n            )\n            \n            # Evaluate model\n            val_loss, val_accuracy, val_auc, val_precision, val_recall = model.evaluate(val_gen_fold)\n            print(f\"Fold {fold + 1} - Validation Loss: {val_loss:.4f}, \"\n                  f\"Accuracy: {val_accuracy:.4f}, AUC: {val_auc:.4f}, \"\n                  f\"Precision: {val_precision:.4f}, Recall: {val_recall:.4f}\")\n            \n            # Calculate F1-score\n            f1_score = 2 * (val_precision * val_recall) / (val_precision + val_recall + K.epsilon())\n            print(f\"F1-score: {f1_score:.4f}\")\n            \n            # Keep track of the best model\n            if val_auc > best_auc:\n                best_auc = val_auc\n                best_model = model\n    \n    else:\n        # If a validation generator is provided, use it directly\n        model = create_model((224, 224, 3), train_generator.data.shape[1] - 2)\n        \n        # Define callbacks\n        model_checkpoint = ModelCheckpoint(\n            'best_model.keras',\n            monitor='val_auc',\n            mode='max',\n            save_best_only=True,\n            verbose=1\n        )\n        callbacks = [\n            EarlyStopping(patience=10, restore_best_weights=True),\n            ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6),\n            model_checkpoint\n        ]\n        \n        # Train model\n        history = model.fit(\n            train_generator,\n            validation_data=val_generator,\n            epochs=epochs,\n            callbacks=callbacks\n        )\n        \n        best_model = model\n    \n    # Save the overall best model\n    best_model.save('best_model_overall.keras')\n    print(f\"Best model saved with validation AUC: {best_auc:.4f}\")\n    \n    return best_model","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:57:43.884256Z","iopub.execute_input":"2024-08-19T13:57:43.884563Z","iopub.status.idle":"2024-08-19T13:57:43.898574Z","shell.execute_reply.started":"2024-08-19T13:57:43.884490Z","shell.execute_reply":"2024-08-19T13:57:43.897826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_class_distribution(data):\n    class_counts = data['target'].value_counts()\n    class_percentages = class_counts / len(data) * 100\n    \n    print(\"Class Distribution:\")\n    for class_label, count in class_counts.items():\n        percentage = class_percentages[class_label]\n        print(f\"Class {class_label}: {count} samples ({percentage:.2f}%)\")\n    \n    imbalance_ratio = class_counts.max() / class_counts.min()\n    print(f\"\\nImbalance Ratio: {imbalance_ratio:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:57:43.899482Z","iopub.execute_input":"2024-08-19T13:57:43.899790Z","iopub.status.idle":"2024-08-19T13:57:43.910732Z","shell.execute_reply.started":"2024-08-19T13:57:43.899767Z","shell.execute_reply":"2024-08-19T13:57:43.909975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_on_test_set(model, test_gen, test_metadata):\n    all_predictions = []\n    for i in range(len(test_gen)):\n        try:\n            batch = test_gen[i]\n            predictions = model.predict_on_batch(batch)\n            all_predictions.append(predictions)\n            print(f\"Successfully predicted batch {i} with shape {predictions.shape}\")\n        except Exception as e:\n            print(f\"Error predicting batch {i}: {e}\")\n    \n    print(f\"Total batches processed: {len(test_gen)}\")\n    print(f\"Number of successful predictions: {len(all_predictions)}\")\n    \n    if not all_predictions:\n        raise ValueError(\"No predictions were made successfully.\")\n    \n    predictions = np.concatenate(all_predictions).flatten()\n    \n    print(f\"Final predictions shape: {predictions.shape}\")\n    \n    # Create submission DataFrame\n    submission = pd.DataFrame({\n        'isic_id': test_metadata['isic_id'],\n        'target': predictions\n    })\n    \n    # Save submission file\n    submission.to_csv('submission.csv', index=False)\n    print(\"Predictions saved to 'submission.csv'\")\n    \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:57:43.911938Z","iopub.execute_input":"2024-08-19T13:57:43.912901Z","iopub.status.idle":"2024-08-19T13:57:43.920980Z","shell.execute_reply.started":"2024-08-19T13:57:43.912851Z","shell.execute_reply":"2024-08-19T13:57:43.920263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Load and preprocess training data\n    train_data, train_hdf5_path = load_and_preprocess_data(train_metadata_path, train_image_hdf5_path, is_train=True)\n    \n    print(\"Original data distribution:\")\n    check_class_distribution(train_data)\n    \n    # Balance the dataset (if needed)\n    balanced_data = balance_dataset(train_data)\n    \n    print(\"\\nBalanced data distribution:\")\n    check_class_distribution(balanced_data)\n    \n    # Create data generators\n    train_generator = HDF5DataGenerator(balanced_data, train_hdf5_path)\n    \n    # Train model\n    best_model = train_model(train_generator)\n    \n    # Load and preprocess test data\n    test_data, test_hdf5_path = load_and_preprocess_data(test_metadata_path, test_image_hdf5_path, is_train=False)\n    \n    # Create test generator\n    test_generator = HDF5DataGenerator(test_data, test_hdf5_path)\n    \n    # Make predictions on test set\n    predictions = evaluate_on_test_set(best_model, test_generator, test_data)\n    \n    print(\"Predictions saved to 'submission.csv'\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:57:43.923674Z","iopub.execute_input":"2024-08-19T13:57:43.923961Z"},"trusted":true},"execution_count":null,"outputs":[]}]}