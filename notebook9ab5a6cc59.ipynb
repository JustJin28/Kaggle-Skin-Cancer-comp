{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44bd867d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-29T18:01:01.236811Z",
     "iopub.status.busy": "2024-08-29T18:01:01.236401Z",
     "iopub.status.idle": "2024-08-29T18:01:17.618681Z",
     "shell.execute_reply": "2024-08-29T18:01:17.617218Z"
    },
    "papermill": {
     "duration": 16.392549,
     "end_time": "2024-08-29T18:01:17.621283",
     "exception": false,
     "start_time": "2024-08-29T18:01:01.228734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 18:01:04.362062: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-29 18:01:04.362206: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-29 18:01:04.533002: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import gc\n",
    "import logging\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, auc\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Concatenate, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "import h5py\n",
    "import math\n",
    "from tensorflow.keras.utils import Progbar\n",
    "import time\n",
    "from tensorflow.keras import mixed_precision\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a4686d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T18:01:17.634275Z",
     "iopub.status.busy": "2024-08-29T18:01:17.633604Z",
     "iopub.status.idle": "2024-08-29T18:01:17.639490Z",
     "shell.execute_reply": "2024-08-29T18:01:17.638361Z"
    },
    "papermill": {
     "duration": 0.014688,
     "end_time": "2024-08-29T18:01:17.641661",
     "exception": false,
     "start_time": "2024-08-29T18:01:17.626973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a78574",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T18:01:17.654247Z",
     "iopub.status.busy": "2024-08-29T18:01:17.653872Z",
     "iopub.status.idle": "2024-08-29T18:01:17.662292Z",
     "shell.execute_reply": "2024-08-29T18:01:17.661199Z"
    },
    "papermill": {
     "duration": 0.017381,
     "end_time": "2024-08-29T18:01:17.664623",
     "exception": false,
     "start_time": "2024-08-29T18:01:17.647242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set memory growth for GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f1094c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T18:01:17.677537Z",
     "iopub.status.busy": "2024-08-29T18:01:17.677148Z",
     "iopub.status.idle": "2024-08-29T18:01:17.682190Z",
     "shell.execute_reply": "2024-08-29T18:01:17.681139Z"
    },
    "papermill": {
     "duration": 0.014008,
     "end_time": "2024-08-29T18:01:17.684476",
     "exception": false,
     "start_time": "2024-08-29T18:01:17.670468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set mixed precision policy\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50c00fbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T18:01:17.696933Z",
     "iopub.status.busy": "2024-08-29T18:01:17.696532Z",
     "iopub.status.idle": "2024-08-29T18:01:17.701377Z",
     "shell.execute_reply": "2024-08-29T18:01:17.700363Z"
    },
    "papermill": {
     "duration": 0.013879,
     "end_time": "2024-08-29T18:01:17.703843",
     "exception": false,
     "start_time": "2024-08-29T18:01:17.689964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# File paths\n",
    "train_metadata_path = '/kaggle/input/isic-2024-challenge/train-metadata.csv'\n",
    "train_image_hdf5_path = '/kaggle/input/isic-2024-challenge/train-image.hdf5'\n",
    "test_metadata_path = '/kaggle/input/isic-2024-challenge/test-metadata.csv'\n",
    "test_image_hdf5_path = '/kaggle/input/isic-2024-challenge/test-image.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4033882b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T18:01:17.716673Z",
     "iopub.status.busy": "2024-08-29T18:01:17.716250Z",
     "iopub.status.idle": "2024-08-29T18:01:17.723308Z",
     "shell.execute_reply": "2024-08-29T18:01:17.722149Z"
    },
    "papermill": {
     "duration": 0.016371,
     "end_time": "2024-08-29T18:01:17.725755",
     "exception": false,
     "start_time": "2024-08-29T18:01:17.709384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image_from_hdf5(hdf5_path, image_id):\n",
    "    with h5py.File(hdf5_path, 'r') as hdf:\n",
    "        # Convert image_id to string if it's not already\n",
    "        image_id = str(image_id)\n",
    "        \n",
    "        # Check if the image_id exists in the file\n",
    "        if image_id not in hdf:\n",
    "            raise KeyError(f\"Image ID {image_id} not found in HDF5 file\")\n",
    "        \n",
    "        # Load the raw data\n",
    "        image_data = hdf[image_id][()]\n",
    "        \n",
    "    # Convert the data to a numpy array\n",
    "    image_array = np.frombuffer(image_data, dtype=np.uint8)\n",
    "    \n",
    "    # Decode the image using OpenCV\n",
    "    image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    # Convert BGR to RGB (OpenCV loads images in BGR format)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4e4cd46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T18:01:17.738650Z",
     "iopub.status.busy": "2024-08-29T18:01:17.738253Z",
     "iopub.status.idle": "2024-08-29T18:01:17.744257Z",
     "shell.execute_reply": "2024-08-29T18:01:17.743087Z"
    },
    "papermill": {
     "duration": 0.015288,
     "end_time": "2024-08-29T18:01:17.746754",
     "exception": false,
     "start_time": "2024-08-29T18:01:17.731466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image, target_size=(224, 224)):\n",
    "    # Resize the image\n",
    "    image_resized = cv2.resize(image, target_size)\n",
    "    \n",
    "    # Normalize the image\n",
    "    image_normalized = image_resized.astype(np.float32) / 255.0\n",
    "    \n",
    "    return image_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0c45ce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T18:01:17.760006Z",
     "iopub.status.busy": "2024-08-29T18:01:17.759586Z",
     "iopub.status.idle": "2024-08-29T18:01:17.777927Z",
     "shell.execute_reply": "2024-08-29T18:01:17.776866Z"
    },
    "papermill": {
     "duration": 0.027768,
     "end_time": "2024-08-29T18:01:17.780134",
     "exception": false,
     "start_time": "2024-08-29T18:01:17.752366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(metadata_path, hdf5_path, is_train=True, train_columns=None, train_encoders=None):\n",
    "    try:\n",
    "        data = pd.read_csv(metadata_path, low_memory=False)\n",
    "\n",
    "        # Handle empty dataset\n",
    "        if data.empty:\n",
    "            raise ValueError(\"Empty dataset\")\n",
    "\n",
    "        # Store 'isic_id' separately\n",
    "        isic_ids = data['isic_id']\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        columns_to_drop = ['patient_id', 'copyright_license', 'attribution', 'image_type', \n",
    "                           'tbp_tile_type', 'lesion_id']\n",
    "        data = data.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "        # Handle missing values in numeric columns\n",
    "        numeric_columns = ['age_approx', 'clin_size_long_diam_mm', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext', \n",
    "                           'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', 'tbp_lv_Lext', \n",
    "                           'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean', 'tbp_lv_deltaA', \n",
    "                           'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', \n",
    "                           'tbp_lv_minorAxisMM', 'tbp_lv_nevi_confidence', 'tbp_lv_norm_border', 'tbp_lv_norm_color', \n",
    "                           'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max', 'tbp_lv_stdL', 'tbp_lv_stdLExt', \n",
    "                           'tbp_lv_symm_2axis', 'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z']\n",
    "\n",
    "        if is_train:\n",
    "            imputer = SimpleImputer(strategy='median')\n",
    "            data[numeric_columns] = imputer.fit_transform(data[numeric_columns])\n",
    "        else:\n",
    "            # Use the imputer fitted on training data\n",
    "            data[numeric_columns] = train_encoders['imputer'].transform(data[numeric_columns])\n",
    "\n",
    "        # Encode categorical variables\n",
    "        categorical_columns = ['sex', 'anatom_site_general', 'tbp_lv_location', 'tbp_lv_location_simple']\n",
    "        if is_train:\n",
    "            categorical_columns.extend(['iddx_full', 'iddx_1', 'iddx_2', 'iddx_3', 'iddx_4', 'iddx_5'])\n",
    "            label_encoders = {}\n",
    "            for col in categorical_columns:\n",
    "                if col in data.columns:\n",
    "                    le = LabelEncoder()\n",
    "                    data[col] = data[col].fillna('Unknown')\n",
    "                    data[col] = le.fit_transform(data[col].astype(str))\n",
    "                    label_encoders[col] = le\n",
    "        else:\n",
    "            # Use the label encoders fitted on training data\n",
    "            for col in categorical_columns:\n",
    "                if col in data.columns:\n",
    "                    data[col] = data[col].fillna('Unknown')\n",
    "                    data[col] = train_encoders['label_encoders'][col].transform(data[col].astype(str))\n",
    "\n",
    "        # One-hot encode relevant categorical variables\n",
    "        categorical_columns_to_onehot = ['sex', 'anatom_site_general', 'tbp_lv_location', 'tbp_lv_location_simple']\n",
    "        if is_train:\n",
    "            data = pd.get_dummies(data, columns=categorical_columns_to_onehot)\n",
    "            train_columns = data.columns\n",
    "        else:\n",
    "            # For test data, add missing columns\n",
    "            for col in train_columns:\n",
    "                if col not in data.columns:\n",
    "                    data[col] = 0\n",
    "            # Ensure test data has the same columns as train data\n",
    "            data = data[train_columns]\n",
    "\n",
    "        # Scale numerical features\n",
    "        if is_train:\n",
    "            scaler = StandardScaler()\n",
    "            data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
    "        else:\n",
    "            # Use the scaler fitted on training data\n",
    "            data[numeric_columns] = train_encoders['scaler'].transform(data[numeric_columns])\n",
    "\n",
    "        if is_train:\n",
    "            # Handle 'mel_mitotic_index' if present\n",
    "            if 'mel_mitotic_index' in data.columns:\n",
    "                mitotic_index_mapping = {\n",
    "                    '<1/mm^2': 0, '0/mm^2': 0, '1/mm^2': 1, '2/mm^2': 2, \n",
    "                    '3/mm^2': 3, '4/mm^2': 4, '>4/mm^2': 5\n",
    "                }\n",
    "                data['mel_mitotic_index'] = data['mel_mitotic_index'].map(mitotic_index_mapping).fillna(-1)\n",
    "\n",
    "            # Handle 'mel_thick_mm' if present\n",
    "            if 'mel_thick_mm' in data.columns:\n",
    "                data['mel_thick_mm'] = pd.to_numeric(data['mel_thick_mm'], errors='coerce').fillna(-1)\n",
    "\n",
    "        # Reset index\n",
    "        data = data.reset_index(drop=True)\n",
    "\n",
    "        # Print column names for debugging\n",
    "        print(f\"{'Train' if is_train else 'Test'} columns:\", data.columns)\n",
    "\n",
    "        if is_train:\n",
    "            train_encoders = {\n",
    "                'imputer': imputer,\n",
    "                'label_encoders': label_encoders,\n",
    "                'scaler': scaler\n",
    "            }\n",
    "            features = data\n",
    "            labels = features.pop('target')\n",
    "            return features, labels, isic_ids, hdf5_path, train_columns, train_encoders\n",
    "        else:\n",
    "            features = data\n",
    "            return features, isic_ids, hdf5_path\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in load_and_preprocess_data: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eb226f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T18:01:17.792593Z",
     "iopub.status.busy": "2024-08-29T18:01:17.792233Z",
     "iopub.status.idle": "2024-08-29T18:01:17.800751Z",
     "shell.execute_reply": "2024-08-29T18:01:17.799597Z"
    },
    "papermill": {
     "duration": 0.017535,
     "end_time": "2024-08-29T18:01:17.803179",
     "exception": false,
     "start_time": "2024-08-29T18:01:17.785644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def augment_image(image):\n",
    "    try:\n",
    "        # Convert to tensor if it's not already\n",
    "        image = tf.convert_to_tensor(image)\n",
    "    \n",
    "        # Random horizontal flip\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        \n",
    "        # Random vertical flip\n",
    "        image = tf.image.random_flip_up_down(image)\n",
    "        \n",
    "        # Random rotation\n",
    "        image = tf.image.rot90(image, k=tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
    "        \n",
    "        # Random brightness adjustment\n",
    "        image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "        \n",
    "        # Random contrast adjustment\n",
    "        image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "        \n",
    "        # Random hue adjustment\n",
    "        image = tf.image.random_hue(image, max_delta=0.2)\n",
    "        \n",
    "        # Random saturation adjustment\n",
    "        image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
    "        \n",
    "        # Ensure pixel values are in [0, 1] range\n",
    "        image = tf.clip_by_value(image, 0, 1)\n",
    "        \n",
    "        # Add more augmentations here\n",
    "        image = tf.image.random_crop(image, [200, 200, 3])\n",
    "        image = tf.image.resize(image, [224, 224])\n",
    "    \n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error in augment_image: {str(e)}\")\n",
    "        return image  # Return original image if augmentation fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b837ea4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T18:01:17.815694Z",
     "iopub.status.busy": "2024-08-29T18:01:17.815288Z",
     "iopub.status.idle": "2024-08-29T18:01:17.826407Z",
     "shell.execute_reply": "2024-08-29T18:01:17.825297Z"
    },
    "papermill": {
     "duration": 0.020113,
     "end_time": "2024-08-29T18:01:17.828800",
     "exception": false,
     "start_time": "2024-08-29T18:01:17.808687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset(features, labels, hdf5_path, batch_size, is_train=True, isic_ids=None):\n",
    "    def generator():\n",
    "        for i in range(len(features)):\n",
    "            try:\n",
    "                img_id = isic_ids.iloc[i] if isic_ids is not None else features.index[i]\n",
    "                img = load_image_from_hdf5(hdf5_path, img_id)\n",
    "                img_processed = preprocess_image(img)\n",
    "                tab_data = features.iloc[i].values\n",
    "                if is_train:\n",
    "                    yield (img_processed, tab_data), labels.iloc[i]\n",
    "                else:\n",
    "                    yield (img_processed, tab_data)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing sample {i} with ID {img_id}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    output_signature = (\n",
    "        (tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),\n",
    "         tf.TensorSpec(shape=(features.shape[1],), dtype=tf.float32)),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "    ) if is_train else (\n",
    "        (tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),\n",
    "         tf.TensorSpec(shape=(features.shape[1],), dtype=tf.float32))\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(buffer_size=len(features)).repeat()\n",
    "\n",
    "    # Materialize a small part of the dataset to ensure it's not empty\n",
    "    for _ in dataset.take(1):\n",
    "        break\n",
    "\n",
    "    logger.info(f\"Dataset created successfully with {len(features)} samples\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "193cb5dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T18:01:17.841427Z",
     "iopub.status.busy": "2024-08-29T18:01:17.841063Z",
     "iopub.status.idle": "2024-08-29T18:01:17.848241Z",
     "shell.execute_reply": "2024-08-29T18:01:17.847233Z"
    },
    "papermill": {
     "duration": 0.016018,
     "end_time": "2024-08-29T18:01:17.850415",
     "exception": false,
     "start_time": "2024-08-29T18:01:17.834397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Focal Loss implementation\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1 + K.epsilon())) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02b6f39f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T18:01:17.863383Z",
     "iopub.status.busy": "2024-08-29T18:01:17.862999Z",
     "iopub.status.idle": "2024-08-29T18:01:17.873856Z",
     "shell.execute_reply": "2024-08-29T18:01:17.872733Z"
    },
    "papermill": {
     "duration": 0.02,
     "end_time": "2024-08-29T18:01:17.876121",
     "exception": false,
     "start_time": "2024-08-29T18:01:17.856121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def balance_dataset(features, labels, isic_ids, max_ratio=100, min_samples=1000):\n",
    "    print(f\"Starting balance_dataset with {len(features)} samples\")\n",
    "    majority_class = features[labels == 0]\n",
    "    minority_class = features[labels == 1]\n",
    "    majority_isic_ids = isic_ids[labels == 0]\n",
    "    minority_isic_ids = isic_ids[labels == 1]\n",
    "    print(f\"Majority class: {len(majority_class)}, Minority class: {len(minority_class)}\")\n",
    "    \n",
    "    # Determine the number of samples for each class\n",
    "    n_minority = max(min_samples, len(minority_class))\n",
    "    n_majority = min(len(majority_class), n_minority * max_ratio)\n",
    "    \n",
    "    print(f\"Target majority samples: {n_majority}, Target minority samples: {n_minority}\")\n",
    "    \n",
    "    # Oversample minority class\n",
    "    if len(minority_class) < n_minority:\n",
    "        minority_resampled, minority_labels_resampled, minority_isic_ids_resampled = resample(\n",
    "            minority_class, \n",
    "            pd.Series([1] * len(minority_class)),\n",
    "            minority_isic_ids,\n",
    "            n_samples=n_minority, \n",
    "            random_state=42\n",
    "        )\n",
    "    else:\n",
    "        minority_resampled, minority_labels_resampled, minority_isic_ids_resampled = minority_class, pd.Series([1] * len(minority_class)), minority_isic_ids\n",
    "    \n",
    "    # Undersample majority class\n",
    "    if len(majority_class) > n_majority:\n",
    "        majority_resampled, majority_labels_resampled, majority_isic_ids_resampled = resample(\n",
    "            majority_class, \n",
    "            pd.Series([0] * len(majority_class)),\n",
    "            majority_isic_ids,\n",
    "            n_samples=n_majority, \n",
    "            random_state=42\n",
    "        )\n",
    "    else:\n",
    "        majority_resampled, majority_labels_resampled, majority_isic_ids_resampled = majority_class, pd.Series([0] * len(majority_class)), majority_isic_ids\n",
    "    \n",
    "    balanced_features = pd.concat([majority_resampled, minority_resampled])\n",
    "    balanced_labels = pd.concat([majority_labels_resampled, minority_labels_resampled])\n",
    "    balanced_isic_ids = pd.concat([majority_isic_ids_resampled, minority_isic_ids_resampled])\n",
    "    \n",
    "    print(f\"Balanced dataset size: {len(balanced_features)}\")\n",
    "    return balanced_features.reset_index(drop=True), balanced_labels.reset_index(drop=True), balanced_isic_ids.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a58114f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T18:01:17.889160Z",
     "iopub.status.busy": "2024-08-29T18:01:17.888762Z",
     "iopub.status.idle": "2024-08-29T18:01:17.896773Z",
     "shell.execute_reply": "2024-08-29T18:01:17.895629Z"
    },
    "papermill": {
     "duration": 0.017123,
     "end_time": "2024-08-29T18:01:17.899022",
     "exception": false,
     "start_time": "2024-08-29T18:01:17.881899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(train_data, train_hdf5_path, val_data, val_hdf5_path):\n",
    "    learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "    batch_sizes = [16, 32, 64]\n",
    "    best_auc = 0\n",
    "    best_params = {}\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        for bs in batch_sizes:\n",
    "            print(f\"Training with learning rate: {lr}, batch size: {bs}\")\n",
    "            model = train_model(train_data, train_hdf5_path, val_data, val_hdf5_path, n_splits=5, epochs=30, batch_size=bs, learning_rate=lr)\n",
    "            \n",
    "            # Evaluate the model\n",
    "            val_gen = HDF5DataGenerator(val_data, val_hdf5_path, batch_size=bs)\n",
    "            val_dataset = create_dataset(val_gen, val_data, bs)\n",
    "            _, _, val_auc, _, _ = model.evaluate(val_dataset)\n",
    "            \n",
    "            if val_auc > best_auc:\n",
    "                best_auc = val_auc\n",
    "                best_params = {'learning_rate': lr, 'batch_size': bs}\n",
    "    \n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50007bb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T18:01:17.912016Z",
     "iopub.status.busy": "2024-08-29T18:01:17.911600Z",
     "iopub.status.idle": "2024-08-29T18:01:17.921092Z",
     "shell.execute_reply": "2024-08-29T18:01:17.919861Z"
    },
    "papermill": {
     "duration": 0.01852,
     "end_time": "2024-08-29T18:01:17.923218",
     "exception": false,
     "start_time": "2024-08-29T18:01:17.904698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(img_shape, tab_shape, learning_rate=1e-4):\n",
    "    print(f\"Creating model with img_shape={img_shape}, tab_shape={tab_shape}\")\n",
    "    base_model = tf.keras.applications.EfficientNetB0(input_shape=img_shape, include_top=False, weights=None)\n",
    "    print(\"Base model created without pre-trained weights\")\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    print(\"Global average pooling added\")\n",
    "    \n",
    "    # Tabular input branch\n",
    "    tab_input = Input(shape=(tab_shape,))\n",
    "    y = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(tab_input)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dropout(0.5)(y)\n",
    "\n",
    "    # Combine branches\n",
    "    combined = Concatenate()([x, y])\n",
    "    z = Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(combined)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = Dropout(0.5)(z)\n",
    "    output = Dense(1, activation='sigmoid', dtype='float32')(z)\n",
    "\n",
    "    model = Model(inputs=[base_model.input, tab_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss=focal_loss(alpha=.25, gamma=2),\n",
    "                  metrics=['accuracy', AUC(name='auc'), Precision(name='precision'), Recall(name='recall')])\n",
    "    print(\"Model compiled\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4779ef26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T18:01:17.936621Z",
     "iopub.status.busy": "2024-08-29T18:01:17.935721Z",
     "iopub.status.idle": "2024-08-29T18:01:17.950592Z",
     "shell.execute_reply": "2024-08-29T18:01:17.949383Z"
    },
    "papermill": {
     "duration": 0.024107,
     "end_time": "2024-08-29T18:01:17.953000",
     "exception": false,
     "start_time": "2024-08-29T18:01:17.928893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(features, labels, hdf5_path, n_splits=5, epochs=30, batch_size=16, learning_rate=1e-4):\n",
    "    print(f\"Starting model training with {len(features)} samples\")\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_models = []\n",
    "    fold_scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(features, labels)):\n",
    "        print(f\"Processing fold {fold + 1}/{n_splits}\")\n",
    "        \n",
    "        train_features = features.iloc[train_idx]\n",
    "        train_labels = labels.iloc[train_idx]\n",
    "        val_features = features.iloc[val_idx]\n",
    "        val_labels = labels.iloc[val_idx]\n",
    "        \n",
    "        print(f\"Creating train dataset for fold {fold + 1}\")\n",
    "        train_dataset = create_dataset(train_features, train_labels, hdf5_path, batch_size=batch_size, is_train=True)\n",
    "        print(f\"Creating validation dataset for fold {fold + 1}\")\n",
    "        val_dataset = create_dataset(val_features, val_labels, hdf5_path, batch_size=batch_size, is_train=True)\n",
    "        \n",
    "        print(f\"Creating model for fold {fold + 1}\")\n",
    "        model = create_model((224, 224, 3), features.shape[1], learning_rate=learning_rate)\n",
    "        \n",
    "        # Define callbacks\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            f'best_model_fold_{fold+1}.keras',\n",
    "            monitor='val_auc',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=10, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6),\n",
    "            model_checkpoint\n",
    "        ]\n",
    "        \n",
    "        # Calculate steps per epoch\n",
    "        steps_per_epoch = math.ceil(len(train_features) / batch_size)\n",
    "        validation_steps = math.ceil(len(val_features) / batch_size)\n",
    "        \n",
    "        print(f\"Starting training for fold {fold + 1}\")\n",
    "        try:\n",
    "            history = model.fit(\n",
    "                train_dataset,\n",
    "                validation_data=val_dataset,\n",
    "                epochs=epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                validation_steps=validation_steps,\n",
    "                callbacks=callbacks,\n",
    "                verbose=1  # Add this to see training progress\n",
    "            )\n",
    "            print(f\"Fold {fold + 1} training completed\")\n",
    "            \n",
    "            # Evaluate model\n",
    "            print(f\"Evaluating model for fold {fold + 1}\")\n",
    "            val_loss, val_accuracy, val_auc, val_precision, val_recall = model.evaluate(val_dataset, steps=validation_steps)\n",
    "            print(f\"Fold {fold + 1} - Validation Loss: {val_loss:.4f}, \"\n",
    "                  f\"Accuracy: {val_accuracy:.4f}, AUC: {val_auc:.4f}, \"\n",
    "                  f\"Precision: {val_precision:.4f}, Recall: {val_recall:.4f}\")\n",
    "            \n",
    "            # Calculate F1-score\n",
    "            f1_score = 2 * (val_precision * val_recall) / (val_precision + val_recall + K.epsilon())\n",
    "            print(f\"F1-score: {f1_score:.4f}\")\n",
    "            \n",
    "            fold_models.append(model)\n",
    "            fold_scores.append(val_auc)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during training in fold {fold + 1}: {str(e)}\")\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "        finally:\n",
    "            # Clear GPU memory after training\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "    \n",
    "    # Print overall results\n",
    "    print(\"\\nCross-validation results:\")\n",
    "    for fold, score in enumerate(fold_scores):\n",
    "        print(f\"Fold {fold + 1}: AUC = {score:.4f}\")\n",
    "    print(f\"Mean AUC: {np.mean(fold_scores):.4f} (+/- {np.std(fold_scores):.4f})\")\n",
    "    \n",
    "    return fold_models, fold_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a955e667",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T18:01:17.966365Z",
     "iopub.status.busy": "2024-08-29T18:01:17.965709Z",
     "iopub.status.idle": "2024-08-29T18:01:17.971453Z",
     "shell.execute_reply": "2024-08-29T18:01:17.970228Z"
    },
    "papermill": {
     "duration": 0.014838,
     "end_time": "2024-08-29T18:01:17.973719",
     "exception": false,
     "start_time": "2024-08-29T18:01:17.958881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ensemble_predict(models, test_dataset):\n",
    "    all_predictions = []\n",
    "    for model in models:\n",
    "        predictions = model.predict(test_dataset)\n",
    "        all_predictions.append(predictions)\n",
    "    \n",
    "    # Average predictions from all models\n",
    "    ensemble_predictions = np.mean(all_predictions, axis=0)\n",
    "    return ensemble_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc9fd902",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T18:01:17.986979Z",
     "iopub.status.busy": "2024-08-29T18:01:17.986582Z",
     "iopub.status.idle": "2024-08-29T18:01:17.994429Z",
     "shell.execute_reply": "2024-08-29T18:01:17.993390Z"
    },
    "papermill": {
     "duration": 0.017068,
     "end_time": "2024-08-29T18:01:17.996570",
     "exception": false,
     "start_time": "2024-08-29T18:01:17.979502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_class_distribution(data):\n",
    "    print(\"Inside check_class_distribution function\")\n",
    "    print(f\"Data type: {type(data)}\")\n",
    "    print(f\"Data shape or length: {data.shape if hasattr(data, 'shape') else len(data)}\")\n",
    "    \n",
    "    if isinstance(data, pd.DataFrame) and 'target' in data.columns:\n",
    "        class_counts = data['target'].value_counts()\n",
    "    elif isinstance(data, pd.Series):\n",
    "        class_counts = data.value_counts()\n",
    "    else:\n",
    "        print(\"Unexpected data format in check_class_distribution\")\n",
    "        return\n",
    "\n",
    "    class_percentages = class_counts / len(data) * 100\n",
    "    \n",
    "    print(\"Class Distribution:\")\n",
    "    for class_label, count in class_counts.items():\n",
    "        percentage = class_percentages[class_label]\n",
    "        print(f\"Class {class_label}: {count} samples ({percentage:.2f}%)\")\n",
    "    \n",
    "    if len(class_counts) > 1:\n",
    "        imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "        print(f\"\\nImbalance Ratio: {imbalance_ratio:.2f}\")\n",
    "    else:\n",
    "        print(\"\\nOnly one class present, cannot calculate imbalance ratio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d04acac9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T18:01:18.009935Z",
     "iopub.status.busy": "2024-08-29T18:01:18.009526Z",
     "iopub.status.idle": "2024-08-29T18:01:18.018290Z",
     "shell.execute_reply": "2024-08-29T18:01:18.017137Z"
    },
    "papermill": {
     "duration": 0.01804,
     "end_time": "2024-08-29T18:01:18.020472",
     "exception": false,
     "start_time": "2024-08-29T18:01:18.002432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_on_test_set(model, test_gen, test_metadata):\n",
    "    all_predictions = []\n",
    "    for i in range(len(test_gen)):\n",
    "        try:\n",
    "            batch = test_gen[i]\n",
    "            # Print batch information for debugging\n",
    "            print(f\"Batch {i} shapes - Image: {batch[0]['image_input'].shape}, Tabular: {batch[0]['tabular_input'].shape}\")\n",
    "            predictions = model.predict(batch[0], verbose=0)\n",
    "            all_predictions.append(predictions)\n",
    "            print(f\"Successfully predicted batch {i} with shape {predictions.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting batch {i}: {str(e)}\")\n",
    "            # Print more detailed error information\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "    \n",
    "    print(f\"Total batches processed: {len(test_gen)}\")\n",
    "    print(f\"Number of successful predictions: {len(all_predictions)}\")\n",
    "    \n",
    "    if not all_predictions:\n",
    "        raise ValueError(\"No predictions were made successfully. Check the error messages above for more details.\")\n",
    "    \n",
    "    predictions = np.concatenate(all_predictions).flatten()\n",
    "    \n",
    "    print(f\"Final predictions shape: {predictions.shape}\")\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = pd.DataFrame({\n",
    "        'isic_id': test_metadata['isic_id'],\n",
    "        'target': predictions\n",
    "    })\n",
    "    \n",
    "    # Save submission file\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(\"Predictions saved to 'submission.csv'\")\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bc9dda6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T18:01:18.034536Z",
     "iopub.status.busy": "2024-08-29T18:01:18.034164Z",
     "iopub.status.idle": "2024-08-29T18:01:18.039746Z",
     "shell.execute_reply": "2024-08-29T18:01:18.038719Z"
    },
    "papermill": {
     "duration": 0.014784,
     "end_time": "2024-08-29T18:01:18.041916",
     "exception": false,
     "start_time": "2024-08-29T18:01:18.027132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ensemble_predict(models, test_dataset):\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        model_preds = model.predict(test_dataset)\n",
    "        predictions.append(model_preds)\n",
    "    return np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "434e3166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T18:01:18.055368Z",
     "iopub.status.busy": "2024-08-29T18:01:18.054989Z",
     "iopub.status.idle": "2024-08-29T18:01:18.063012Z",
     "shell.execute_reply": "2024-08-29T18:01:18.061875Z"
    },
    "papermill": {
     "duration": 0.017305,
     "end_time": "2024-08-29T18:01:18.065230",
     "exception": false,
     "start_time": "2024-08-29T18:01:18.047925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_on_test_data(model, test_dataset, steps, timeout=3600):\n",
    "    predictions = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        for i, batch in enumerate(test_dataset.take(steps)):\n",
    "            if time.time() - start_time > timeout:\n",
    "                logger.warning(f\"Prediction timed out after {timeout} seconds\")\n",
    "                break\n",
    "            \n",
    "            img_batch, tab_batch = batch\n",
    "            batch_predictions = model.predict([img_batch, tab_batch], verbose=0)\n",
    "            predictions.extend(batch_predictions.flatten())\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                logger.info(f\"Predicted batch {i}/{steps}\")\n",
    "            \n",
    "            # Clear GPU memory if needed\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during prediction: {str(e)}\")\n",
    "    \n",
    "    return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce94023f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T18:01:18.079464Z",
     "iopub.status.busy": "2024-08-29T18:01:18.078623Z",
     "iopub.status.idle": "2024-08-29T18:01:33.012897Z",
     "shell.execute_reply": "2024-08-29T18:01:33.011994Z"
    },
    "papermill": {
     "duration": 14.944234,
     "end_time": "2024-08-29T18:01:33.015493",
     "exception": false,
     "start_time": "2024-08-29T18:01:18.071259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns: Index(['isic_id', 'target', 'age_approx', 'clin_size_long_diam_mm', 'tbp_lv_A',\n",
      "       'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext', 'tbp_lv_C', 'tbp_lv_Cext',\n",
      "       'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', 'tbp_lv_Lext', 'tbp_lv_areaMM2',\n",
      "       'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean', 'tbp_lv_deltaA',\n",
      "       'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB',\n",
      "       'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM',\n",
      "       'tbp_lv_nevi_confidence', 'tbp_lv_norm_border', 'tbp_lv_norm_color',\n",
      "       'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max', 'tbp_lv_stdL',\n",
      "       'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle',\n",
      "       'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z', 'iddx_full', 'iddx_1', 'iddx_2',\n",
      "       'iddx_3', 'iddx_4', 'iddx_5', 'mel_mitotic_index', 'mel_thick_mm',\n",
      "       'tbp_lv_dnn_lesion_confidence', 'sex_0', 'sex_1', 'sex_2',\n",
      "       'anatom_site_general_0', 'anatom_site_general_1',\n",
      "       'anatom_site_general_2', 'anatom_site_general_3',\n",
      "       'anatom_site_general_4', 'anatom_site_general_5', 'tbp_lv_location_0',\n",
      "       'tbp_lv_location_1', 'tbp_lv_location_2', 'tbp_lv_location_3',\n",
      "       'tbp_lv_location_4', 'tbp_lv_location_5', 'tbp_lv_location_6',\n",
      "       'tbp_lv_location_7', 'tbp_lv_location_8', 'tbp_lv_location_9',\n",
      "       'tbp_lv_location_10', 'tbp_lv_location_11', 'tbp_lv_location_12',\n",
      "       'tbp_lv_location_13', 'tbp_lv_location_14', 'tbp_lv_location_15',\n",
      "       'tbp_lv_location_16', 'tbp_lv_location_17', 'tbp_lv_location_18',\n",
      "       'tbp_lv_location_19', 'tbp_lv_location_20', 'tbp_lv_location_simple_0',\n",
      "       'tbp_lv_location_simple_1', 'tbp_lv_location_simple_2',\n",
      "       'tbp_lv_location_simple_3', 'tbp_lv_location_simple_4',\n",
      "       'tbp_lv_location_simple_5', 'tbp_lv_location_simple_6',\n",
      "       'tbp_lv_location_simple_7'],\n",
      "      dtype='object')\n",
      "Inside check_class_distribution function\n",
      "Data type: <class 'pandas.core.series.Series'>\n",
      "Data shape or length: (401059,)\n",
      "Class Distribution:\n",
      "Class 0: 400666 samples (99.90%)\n",
      "Class 1: 393 samples (0.10%)\n",
      "\n",
      "Imbalance Ratio: 1019.51\n",
      "Starting balance_dataset with 401059 samples\n",
      "Majority class: 400666, Minority class: 393\n",
      "Target majority samples: 100000, Target minority samples: 1000\n",
      "Balanced dataset size: 101000\n",
      "Inside check_class_distribution function\n",
      "Data type: <class 'pandas.core.series.Series'>\n",
      "Data shape or length: (101000,)\n",
      "Class Distribution:\n",
      "Class 0: 100000 samples (99.01%)\n",
      "Class 1: 1000 samples (0.99%)\n",
      "\n",
      "Imbalance Ratio: 100.00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        logging.basicConfig(level=logging.DEBUG)\n",
    "        logger = logging.getLogger(__name__)\n",
    "\n",
    "        logger.info(\"Starting data loading and preprocessing...\")\n",
    "        train_features, train_labels, train_isic_ids, train_hdf5_path, train_columns, train_encoders = load_and_preprocess_data(train_metadata_path, train_image_hdf5_path, is_train=True)\n",
    "        logger.info(\"Data loading and preprocessing completed.\")\n",
    "\n",
    "        logger.info(\"Checking original class distribution...\")\n",
    "        check_class_distribution(train_labels)\n",
    "        logger.info(\"Original class distribution check completed.\")\n",
    "\n",
    "        logger.info(\"Starting dataset balancing...\")\n",
    "        balanced_features, balanced_labels, balanced_isic_ids = balance_dataset(train_features, train_labels, train_isic_ids)\n",
    "        logger.info(\"Dataset balancing completed.\")\n",
    "\n",
    "        logger.info(\"Checking balanced class distribution...\")\n",
    "        check_class_distribution(balanced_labels)\n",
    "        logger.info(\"Balanced class distribution check completed.\")\n",
    "\n",
    "        logger.info(\"Creating dataset...\")\n",
    "        train_dataset = create_dataset(balanced_features, balanced_labels, train_hdf5_path, batch_size=16, is_train=True, isic_ids=balanced_isic_ids)\n",
    "        logger.info(\"Dataset creation completed.\")\n",
    "\n",
    "        logger.info(\"Starting model training\")\n",
    "        fold_models, fold_scores = train_model(balanced_features, balanced_isic_ids, balanced_labels, train_hdf5_path, n_splits=5, epochs=30, batch_size=16, learning_rate=1e-4)\n",
    "        logger.info(\"Model training completed\")\n",
    "        \n",
    "        # Process test data\n",
    "        logger.info(\"Processing test data...\")\n",
    "        test_features, test_isic_ids, test_hdf5_path = load_and_preprocess_data(\n",
    "            test_metadata_path, test_image_hdf5_path, \n",
    "            is_train=False, train_columns=train_columns, \n",
    "            train_encoders=train_encoders\n",
    "        )\n",
    "        logger.info(\"Test data processing completed.\")\n",
    "        \n",
    "        # Create test dataset\n",
    "        logger.info(\"Creating test dataset...\")\n",
    "        batch_size = 16\n",
    "        test_dataset = create_dataset(test_features, test_isic_ids, None, test_hdf5_path, batch_size=batch_size, is_train=False)\n",
    "        logger.info(\"Test dataset creation completed.\")\n",
    "        \n",
    "        # Calculate steps for prediction\n",
    "        test_steps = math.ceil(len(test_features) / batch_size)\n",
    "        \n",
    "        # Make ensemble predictions\n",
    "        logger.info(\"Making ensemble predictions...\")\n",
    "        predictions = ensemble_predict(fold_models, test_dataset, test_steps)\n",
    "        logger.info(\"Ensemble predictions completed.\")\n",
    "        \n",
    "        # Handle potential issues with predictions\n",
    "        if len(predictions) == 0:\n",
    "            logger.error(\"No predictions were made\")\n",
    "            predictions = np.zeros(len(test_features))  # Default predictions\n",
    "        elif len(predictions) != len(test_features):\n",
    "            logger.warning(f\"Mismatch in prediction length. Padding with zeros.\")\n",
    "            padding = np.zeros(len(test_features) - len(predictions))\n",
    "            predictions = np.concatenate([predictions, padding])\n",
    "        \n",
    "        # Create submission DataFrame\n",
    "        logger.info(\"Creating submission DataFrame...\")\n",
    "        submission = pd.DataFrame({\n",
    "            'isic_id': test_isic_ids,\n",
    "            'target': predictions\n",
    "        })\n",
    "        \n",
    "        # Handle any potential NaN values\n",
    "        submission['target'] = submission['target'].fillna(0.5)  # Fill NaNs with 0.5\n",
    "        \n",
    "        # Clip predictions to [0, 1] range\n",
    "        submission['target'] = np.clip(submission['target'], 0, 1)\n",
    "        \n",
    "        # Save submission file\n",
    "        submission.to_csv('submission.csv', index=False)\n",
    "        logger.info(\"Predictions saved to 'submission.csv'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())  # Print the full traceback\n",
    "        # Create a default submission in case of critical error\n",
    "        test_data = pd.read_csv(test_metadata_path)\n",
    "        default_submission = pd.DataFrame({\n",
    "            'isic_id': test_data['isic_id'],\n",
    "            'target': [0.5] * len(test_data)  # Default prediction of 0.5 for all samples\n",
    "        })\n",
    "        default_submission.to_csv('submission.csv', index=False)\n",
    "        logger.info(\"Default submission saved due to error\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9094797,
     "sourceId": 63056,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 37.564289,
   "end_time": "2024-08-29T18:01:35.511485",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-29T18:00:57.947196",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
